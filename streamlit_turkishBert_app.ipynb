{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "6MSEUese3Ly0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a4cc681-c861-4041-9060-7c751f771ad6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbUGaor4xodd",
        "outputId": "d18dbf5c-3b8d-46ba-905c-e6602d3f3570"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors in 1.97s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xh4aRJD3NrS",
        "outputId": "7a3a0091-f980-4fc6-cd6f-f6520ed7ac9d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "104.196.219.103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load model and tokenizer\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    model = BertForSequenceClassification.from_pretrained('mtolgakbabai16tr/bertTurkishModel')\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(\"mtolgakbabai16tr/bertTurkishModel\", max_length=510)\n",
        "    clf = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
        "    return clf\n",
        "\n",
        "clf = load_model()\n",
        "\n",
        "st.title('My AI App')\n",
        "\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "input_text = st.chat_input(\"Write your text\")\n",
        "if input_text:\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": input_text})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(input_text)\n",
        "\n",
        "    with st.chat_message('assistant'):\n",
        "        output = clf(input_text)\n",
        "        answer = output[0]['label']\n",
        "        st.markdown(answer)\n",
        "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": answer})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdCXdDNq3bX7",
        "outputId": "252a952f-9e95-4892-e13c-49f862018272"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx lt --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbuJd3mcxh-F",
        "outputId": "eaeff1dc-1030-4a3b-e060-b6bcb7140935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your url is: https://gold-melons-begin.loca.lt\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://104.196.219.103:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2024-07-18 17:01:56.650047: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-18 17:01:56.650112: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-18 17:01:56.652356: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-18 17:01:56.670963: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-18 17:01:58.539595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "config.json: 100% 924/924 [00:00<00:00, 4.38MB/s]\n",
            "model.safetensors: 100% 443M/443M [00:07<00:00, 60.5MB/s]\n",
            "tokenizer_config.json: 100% 1.27k/1.27k [00:00<00:00, 6.11MB/s]\n",
            "vocab.txt: 100% 251k/251k [00:00<00:00, 4.80MB/s]\n",
            "tokenizer.json: 100% 755k/755k [00:00<00:00, 26.4MB/s]\n",
            "special_tokens_map.json: 100% 125/125 [00:00<00:00, 639kB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!streamlit run app.py & sudo npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "l3V046Hu3Prs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nXhiBaoxwHzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}